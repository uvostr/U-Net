{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdmvAWou5asD"
   },
   "outputs": [],
   "source": [
    "import speedup\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBwqhTuHTY4v"
   },
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mLqaxlPvAqEW"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters):\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"leaky_relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF3OppL0UcFp"
   },
   "outputs": [],
   "source": [
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = tf.keras.layers.MaxPool2D((2, 2))(f)\n",
    "   return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN_5xuYkUm2E"
   },
   "outputs": [],
   "source": [
    "def upsample_block(x, conv_features, n_filters):\n",
    "   x = tf.keras.layers.Conv2DTranspose(n_filters, (3, 3), (2, 2), padding=\"same\")(x)\n",
    "   x = tf.keras.layers.concatenate([x, conv_features])\n",
    "   x = double_conv_block(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awNeMTdAUEYo"
   },
   "outputs": [],
   "source": [
    "imageSize = 512\n",
    "m = 3\n",
    "\n",
    "def Generator():\n",
    "    inputs = tf.keras.Input(shape=(imageSize, imageSize, m))\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "\n",
    "    bottleneck = double_conv_block(p4, 1024)\n",
    "\n",
    "    u6 = upsample_block(bottleneck, f4, 512)\n",
    "    u7 = upsample_block(u6, f3, 256)\n",
    "    u8 = upsample_block(u7, f2, 128)\n",
    "    u9 = upsample_block(u8, f1, 64)\n",
    "    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding = \"same\")(u9)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"generator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tceu77gYOAWO"
   },
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "  inp = tf.keras.Input(shape=(imageSize, imageSize, m), name='input_image')\n",
    "  tar = tf.keras.Input(shape=(imageSize, imageSize, m), name='target_image')\n",
    "  x = tf.keras.layers.concatenate([inp, tar])\n",
    "\n",
    "  f1, p1 = downsample_block(x, 64)\n",
    "  f2, p2 = downsample_block(p1, 128)\n",
    "  f3, p3 = downsample_block(p2, 256)\n",
    "  f4, p4 = downsample_block(p3, 3)\n",
    "\n",
    "  outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding = \"same\")(p4)\n",
    "  return tf.keras.Model([inp, tar], outputs, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwAwrclCE5TA",
    "outputId": "058c9463-ac0a-4a93-da86-f46fe508e64a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " target_image (InputLayer)      [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 512, 512, 6)  0           ['input_image[0][0]',            \n",
      "                                                                  'target_image[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 512, 512, 64  3520        ['concatenate_8[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 512, 512, 64  256        ['conv2d_26[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 512, 512, 64  36928       ['batch_normalization_20[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 512, 512, 64  256        ['conv2d_27[0][0]']              \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 256, 256, 64  0          ['batch_normalization_21[0][0]'] \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 256, 256, 12  73856       ['max_pooling2d_10[0][0]']       \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 256, 256, 12  512        ['conv2d_28[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 256, 256, 12  147584      ['batch_normalization_22[0][0]'] \n",
      "                                8)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 256, 256, 12  512        ['conv2d_29[0][0]']              \n",
      " ormalization)                  8)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 128, 128, 12  0          ['batch_normalization_23[0][0]'] \n",
      " )                              8)                                                                \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 128, 128, 25  295168      ['max_pooling2d_11[0][0]']       \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 128, 128, 25  1024       ['conv2d_30[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 128, 128, 25  590080      ['batch_normalization_24[0][0]'] \n",
      "                                6)                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 128, 128, 25  1024       ['conv2d_31[0][0]']              \n",
      " ormalization)                  6)                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 64, 64, 256)  0          ['batch_normalization_25[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 64, 64, 3)    6915        ['max_pooling2d_12[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 64, 64, 3)   12          ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 64, 64, 3)    84          ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 64, 64, 3)   12          ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 32, 32, 3)   0           ['batch_normalization_27[0][0]'] \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 32, 32, 3)    12          ['max_pooling2d_13[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,157,755\n",
      "Trainable params: 1,155,951\n",
      "Non-trainable params: 1,804\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_instance = Discriminator()\n",
    "model_instance.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLoTQScKX4u_"
   },
   "outputs": [],
   "source": [
    "image_path = '/content/drive/MyDrive/source2'\n",
    "models_path = '/content/drive/MyDrive/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYo1vlvITGZz"
   },
   "outputs": [],
   "source": [
    "LAMBDA = 1\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()#from_logits=True)\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  l1_loss = tf.reduce_mean(tf.abs(tf.cast(target, tf.float32) - tf.cast(gen_output, tf.float32)))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39oUpeHXYIH2"
   },
   "outputs": [],
   "source": [
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "\n",
    "    def compile(self, discriminator_optimizer, generator_optimizer, discriminator_loss, generator_loss):\n",
    "        super(GAN, self).compile()\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_loss = discriminator_loss\n",
    "        self.generator_loss = generator_loss\n",
    "\n",
    "    def call(self, data, training=False):\n",
    "        pass\n",
    "\n",
    "    def train_step(self, data):\n",
    "\n",
    "      input_image, target_image = data\n",
    "\n",
    "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generator_output = self.generator(input_image)\n",
    "        discriminator_real_output = self.discriminator([input_image, target_image])\n",
    "        discriminator_generated_output = self.discriminator([input_image, generator_output])\n",
    "                                                              \n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = self.generator_loss(discriminator_generated_output, generator_output, target_image)\n",
    "        disc_loss = self.discriminator_loss(discriminator_real_output, discriminator_generated_output)\n",
    "      \n",
    "      generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          self.generator.trainable_variables)\n",
    "      discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                          self.discriminator.trainable_variables)\n",
    "\n",
    "      self.generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          self.generator.trainable_variables))\n",
    "      self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              self.discriminator.trainable_variables))\n",
    "      return {\"generator_loss\": gen_total_loss, \"discriminator_loss\": disc_loss}\n",
    "\n",
    "\n",
    "    def test_step(self, data):\n",
    "\n",
    "      input_image, target_image = data\n",
    "\n",
    "      with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generator_output = self.generator(input_image)\n",
    "        discriminator_real_output = self.discriminator([input_image, target_image])\n",
    "        discriminator_generated_output = self.discriminator([input_image, generator_output])\n",
    "                                                              \n",
    "        gen_total_loss, gen_gan_loss, gen_l1_loss = self.generator_loss(discriminator_generated_output, generator_output, target_image)\n",
    "        disc_loss = self.discriminator_loss(discriminator_real_output, discriminator_generated_output)\n",
    "        \n",
    "      generator_gradients = gen_tape.gradient(gen_total_loss,\n",
    "                                          self.generator.trainable_variables)\n",
    "      discriminator_gradients = disc_tape.gradient(disc_loss,\n",
    "                                          self.discriminator.trainable_variables)\n",
    "\n",
    "      self.generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          self.generator.trainable_variables))\n",
    "      self.discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              self.discriminator.trainable_variables))\n",
    "      return {\"generator_loss\": gen_total_loss, \"discriminator_loss\": disc_loss}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUUhfYyC77WI",
    "outputId": "24ab5264-e154-4243-ad71-4fba0ced0a3b"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "from speedup import generate_out_images3\n",
    "import numpy as np\n",
    "from random import randint, uniform\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "\n",
    "source_num = 2799\n",
    "dim = 512\n",
    "stationary_defocus = 0.05\n",
    "\n",
    "\n",
    "def gen():\n",
    "    while True:\n",
    "        layer1_number = randint(0, source_num)\n",
    "        layer2_number = randint(0, source_num)\n",
    "        layer3_number = randint(0, source_num)\n",
    "\n",
    "        src1 = imageio.imread(image_path + '/image' + str(layer1_number).zfill(4) + '.png')\n",
    "        src2 = imageio.imread(image_path + '/image' + str(layer2_number).zfill(4) + '.png')\n",
    "        src3 = imageio.imread(image_path + '/image' + str(layer3_number).zfill(4) + '.png')\n",
    "        src = np.zeros((dim, dim, m), np.double)\n",
    "        src[:, :, 0] = src1[:, :, 0]\n",
    "        src[:, :, 1] = src2[:, :, 0]\n",
    "        src[:, :, 2] = src3[:, :, 0]\n",
    "        src = src - np.amin(src)\n",
    "        src = src / np.amax(src)\n",
    "\n",
    "        w = uniform(0.05, 0.5) \n",
    "        \n",
    "        a_10 = uniform(-1e3, 1e3)\n",
    "        a_01 = uniform(-1e3, 1e3)\n",
    "        b_20 = uniform(1, 1.5)\n",
    "        b_11 = uniform(-0.1, 0.1)\n",
    "        b_02 = uniform(1, 1.5)\n",
    "        c_30 = uniform(-1.5e-6, 1.5e-6)\n",
    "        c_21 = uniform(-2e-6, 2e-6)\n",
    "        c_12 = uniform(-2e-6, 2e-6)\n",
    "        c_03 = uniform(-1.5e-6, 1.5e-6)\n",
    "\n",
    "        out = generate_out_images3(dim, m, w, stationary_defocus, a_10, a_01, b_20, b_11, b_02, c_30, c_21, c_12, c_03, src)[1]\n",
    "\n",
    "        out = out / np.amax(out)\n",
    "\n",
    "        src[src > 0] = 1.\n",
    "\n",
    "        yield (out, src)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam()\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "tr_dataset = tf.data.Dataset.from_generator(\n",
    "     gen, (tf.float64, tf.float64), (tf.TensorShape([dim, dim, m]), tf.TensorShape([dim, dim, m])))\\\n",
    "    .batch(batch_size=2).prefetch(buffer_size=8)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "     gen, (tf.float64, tf.float64), (tf.TensorShape([dim, dim, m]), tf.TensorShape([dim, dim, m])))\\\n",
    "    .take(count=128).cache().batch(batch_size=2)\n",
    "\n",
    "metric = 'val_generator_loss'\n",
    "save_best_callback = tf.keras.callbacks.ModelCheckpoint(models_path + 'bestmodel_gan.hdf5',\n",
    "                                                        save_weights_only=True, save_best_only=True, verbose=True, monitor = metric)\n",
    "csv_logger_callback = tf.keras.callbacks.CSVLogger(models_path + 'log_gan.csv')\n",
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, min_delta=5e-4, patience=5, monitor = metric)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=25, monitor = metric)\n",
    "\n",
    "discriminator = Discriminator()\n",
    "generator = Generator()\n",
    "model_instance = GAN(discriminator, generator)\n",
    "model_instance.compile(discriminator_optimizer=discriminator_optimizer,\n",
    "        generator_optimizer=generator_optimizer,\n",
    "        discriminator_loss = discriminator_loss,\n",
    "        generator_loss = generator_loss, )\n",
    "\n",
    "model_instance.fit(x=tr_dataset, validation_data=val_dataset, verbose=1, validation_steps=64,\n",
    "                   steps_per_epoch=256, epochs=1,\n",
    "                   callbacks=[save_best_callback, csv_logger_callback, lr_reduce_callback, early_stop_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
