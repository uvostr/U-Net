{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pdmvAWou5asD"
   },
   "outputs": [],
   "source": [
    "import speedup\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBwqhTuHTY4v"
   },
   "outputs": [],
   "source": [
    "def double_conv_block(x, n_filters):\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF3OppL0UcFp"
   },
   "outputs": [],
   "source": [
    "def downsample_block(x, n_filters):\n",
    "   f = double_conv_block(x, n_filters)\n",
    "   p = tf.keras.layers.MaxPool2D((2, 2))(f)\n",
    "   return f, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znaLyjxkgy0E"
   },
   "outputs": [],
   "source": [
    "def pooling_block(x, n_filters, max_pool_size):\n",
    "   x = tf.keras.layers.MaxPool2D((max_pool_size, max_pool_size))(x)\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJgbZGl_iO7G"
   },
   "outputs": [],
   "source": [
    "def conv_block(x, n_filters):\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BN_5xuYkUm2E"
   },
   "outputs": [],
   "source": [
    "def upsample_block(x, n_filters, strides_size):\n",
    "   x = tf.keras.layers.Conv2DTranspose(n_filters, (int(1.5 * strides_size), int(1.5 * strides_size)), (strides_size, strides_size), padding=\"same\")(x)\n",
    "   x = tf.keras.layers.Conv2D(n_filters, (3, 3), activation = \"relu\", kernel_initializer = \"he_normal\", padding=\"same\")(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5a5HQsMFjM_H"
   },
   "outputs": [],
   "source": [
    "def concatination_block(x, n_filters):\n",
    "  x = tf.keras.layers.concatenate(x, axis=3)\n",
    "  x = conv_block(x, n_filters)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awNeMTdAUEYo"
   },
   "outputs": [],
   "source": [
    "imageSize = 512\n",
    "m = 3\n",
    "\n",
    "def constructModel():\n",
    "    inputs = tf.keras.Input(shape=(imageSize, imageSize, m))\n",
    "    f1, p1 = downsample_block(inputs, 64)\n",
    "    f2, p2 = downsample_block(p1, 128)\n",
    "    f3, p3 = downsample_block(p2, 256)\n",
    "    f4, p4 = downsample_block(p3, 512)\n",
    "    \n",
    "\n",
    "    hd5 = double_conv_block(p4, 1024)\n",
    "\n",
    "\n",
    "    h14 = pooling_block(f1, 64, 8)\n",
    "    h24 = pooling_block(f2, 64, 4)\n",
    "    h34 = pooling_block(f3, 64, 2)\n",
    "    h44 = conv_block(f4, 64)\n",
    "    h54 = upsample_block(hd5, 64, 2)\n",
    "    hd4 = concatination_block([h14, h24, h34, h44, h54], 320)\n",
    "\n",
    "\n",
    "    h13 = pooling_block(f1, 64, 4)\n",
    "    h23 = pooling_block(f2, 64, 2)\n",
    "    h33 = conv_block(f3, 64)\n",
    "    h43 = upsample_block(hd4, 64, 2)\n",
    "    h53 = upsample_block(hd5, 64, 4)\n",
    "    hd3 = concatination_block([h13, h23, h33, h43, h53], 320)\n",
    "\n",
    "\n",
    "    h12 = pooling_block(f1, 64, 2)\n",
    "    h22 = conv_block(f2, 64)\n",
    "    h32 = upsample_block(hd3, 64, 2)\n",
    "    h42 = upsample_block(hd4, 64, 4)\n",
    "    h52 = upsample_block(hd5, 64, 8)\n",
    "    hd2 = concatination_block([h12, h22, h32, h42, h52], 320)\n",
    "\n",
    "    h11 = conv_block(f1, 64)\n",
    "    h21 = upsample_block(hd2, 64, 2)\n",
    "    h31 = upsample_block(hd3, 64, 4)\n",
    "    h41 = upsample_block(hd4, 64, 8)\n",
    "    h51 = upsample_block(hd5, 64, 16)\n",
    "    hd1 = concatination_block([h11, h21, h31, h41, h51], 320)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid', padding = \"same\")(hd1)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs, name=\"U-Net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60YjGqDZX_Rq",
    "outputId": "4f7273af-6008-40a8-949d-db1ded9e8924"
   },
   "outputs": [],
   "source": [
    "model_instance = constructModel()\n",
    "model_instance.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLoTQScKX4u_"
   },
   "outputs": [],
   "source": [
    "image_path = '/content/drive/MyDrive/source2'\n",
    "models_path = '/content/drive/MyDrive/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUUhfYyC77WI",
    "outputId": "702a1f6b-5c40-4626-e6d6-ff89b7155043"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import tensorflow as tf\n",
    "from speedup import generate_out_images3\n",
    "import numpy as np\n",
    "from random import randint, uniform\n",
    "import imageio\n",
    "import time\n",
    "\n",
    "\n",
    "source_num = 2799\n",
    "dim = 512\n",
    "stationary_defocus = 0.05\n",
    "\n",
    "\n",
    "def gen():\n",
    "    while True:\n",
    "        layer1_number = randint(0, source_num)\n",
    "        layer2_number = randint(0, source_num)\n",
    "        layer3_number = randint(0, source_num)\n",
    "\n",
    "        src1 = imageio.imread(image_path + '/image' + str(layer1_number).zfill(4) + '.png')\n",
    "        src2 = imageio.imread(image_path + '/image' + str(layer2_number).zfill(4) + '.png')\n",
    "        src3 = imageio.imread(image_path + '/image' + str(layer3_number).zfill(4) + '.png')\n",
    "        src = np.zeros((dim, dim, m), np.double)\n",
    "        src[:, :, 0] = src1[:, :, 0]\n",
    "        src[:, :, 1] = src2[:, :, 0]\n",
    "        src[:, :, 2] = src3[:, :, 0]\n",
    "        src = src - np.amin(src)\n",
    "        src = src / np.amax(src)\n",
    "\n",
    "        w = uniform(0.05, 0.5) \n",
    "        \n",
    "        a_10 = uniform(-1e3, 1e3)\n",
    "        a_01 = uniform(-1e3, 1e3)\n",
    "        b_20 = uniform(1, 1.5)\n",
    "        b_11 = uniform(-0.1, 0.1)\n",
    "        b_02 = uniform(1, 1.5)\n",
    "        c_30 = uniform(-1.5e-6, 1.5e-6)\n",
    "        c_21 = uniform(-2e-6, 2e-6)\n",
    "        c_12 = uniform(-2e-6, 2e-6)\n",
    "        c_03 = uniform(-1.5e-6, 1.5e-6)\n",
    "\n",
    "        out = generate_out_images3(dim, m, w, stationary_defocus, a_10, a_01, b_20, b_11, b_02, c_30, c_21, c_12, c_03, src)[1]\n",
    "\n",
    "        out = out / np.amax(out)\n",
    "\n",
    "        src[src > 0] = 1.\n",
    "\n",
    "        yield (out, src)\n",
    "\n",
    "\n",
    "tr_dataset = tf.data.Dataset.from_generator(\n",
    "     gen, (tf.float64, tf.float64), (tf.TensorShape([dim, dim, m]), tf.TensorShape([dim, dim, m])))\\\n",
    "    .batch(batch_size=2).prefetch(buffer_size=8)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "     gen, (tf.float64, tf.float64), (tf.TensorShape([dim, dim, m]), tf.TensorShape([dim, dim, m])))\\\n",
    "    .take(count=128).cache().batch(batch_size=2)\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "save_best_callback = tf.keras.callbacks.ModelCheckpoint(models_path + 'bestmodel_unet_2d_3plus.hdf5',\n",
    "                                                        save_weights_only=True,save_best_only=True, verbose=True)\n",
    "csv_logger_callback = tf.keras.callbacks.CSVLogger(models_path + 'log_unet_2d_3plus.csv')\n",
    "lr_reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, min_delta=5e-4, patience=5)\n",
    "early_stop_callback = tf.keras.callbacks.EarlyStopping(patience=25)\n",
    "\n",
    "model_instance = constructModel()\n",
    "model_instance.compile(loss='binary_crossentropy', optimizer=opt, metrics=['binary_crossentropy', 'mse'])\n",
    "model_instance.fit(x=tr_dataset, validation_data=val_dataset, verbose=1, validation_steps=64,\n",
    "                   steps_per_epoch=256, epochs=200,\n",
    "                   callbacks=[save_best_callback, csv_logger_callback, lr_reduce_callback, early_stop_callback])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
